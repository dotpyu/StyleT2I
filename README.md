# StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis [CVPR 2022]

**TL;DR**: We introduce a new framework, StyleT2I, to achieve compositional and high-fidelity text-to-image synthesis results.

<!-- <img src="images/teaser.jpg" alt="teaser" style="zoom:100%;" /> -->

![abdf](images/teaser.jpg)

**Figure 1.** When the text input contains underrepresented compositions of attributes, e.g., (<span style="color:blue">*he*</span>, <span style="color:magenta">*wearing lipstick*</span>), in the dataset, previous methods [1-3] incorrectly generate the attributes with poor image quality. In contrast, StyleT2I achieves better compositionality and high-fidelity text-to-image synthesis results.

## Paper

**StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis**

[Zhiheng Li](https://zhiheng.li/), [Martin Renqiang Min](https://www.cs.toronto.edu/~cuty/), [Kai Li](http://kailigo.github.io/), [Chenliang Xu](https://www.cs.rochester.edu/~cxu22/)

NEC Laboratories America, University of Rochester

**Contact**: Zhiheng Li (email: zhiheng.li@rochester.edu, homepage: https://zhiheng.li)

## Code
The code will be added soon.


## References

[1] B. Li, X. Qi, T. Lukasiewicz, and P. Torr, “Controllable Text-to-Image Generation,” in NeurIPS, 2019.

[2] S. Ruan et al., “DAE-GAN: Dynamic Aspect-aware GAN for Text-to-Image Synthesis,” in ICCV, 2021.

[3] W. Xia, Y. Yang, J.-H. Xue, and B. Wu, “TediGAN: Text-Guided Diverse Face Image Generation and Manipulation,” in CVPR, 2021.



## Citation

```
@InProceedings{Li_2022_CVPR,
author = {Li, Zhiheng and Min, Martin Renqiang and Li, Kai and Xu, Chenliang},
title = {StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2022}
}
```
